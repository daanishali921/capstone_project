{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343471e8-b215-4bfc-84c3-4eb888c30cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import yfinance as yf\n",
    "\n",
    "import re\n",
    "\n",
    "import time\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8293af13-b4ea-42ad-a7d0-a2fb9326be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep note of directory_name -- it is based on the stock in question of the form tweets_(STOCK_NAME)\n",
    "# example: tweets_GME or tweets_TSLA\n",
    "\n",
    "def scrapeTweets(start, stop, keyword, directory, tweet_limit=1):\n",
    "    if not os.path.exists(directory): # Creates directory in current directory if doesn't already exist\n",
    "        os.mkdir(directory)\n",
    "    \n",
    "    file_path = os.path.join(directory, f'keyword:{keyword}__start:{start}_end:{stop}__limit:{tweet_limit}.csv')\n",
    "    \n",
    "    tweet_list = []\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} since:{start} until:{stop}').get_items()):\n",
    "        if i > tweet_limit:\n",
    "            break\n",
    "        tweet_list.append([tweet.date, # Appending all tweet data into a list of list\n",
    "                           tweet.id, \n",
    "                           tweet.content, \n",
    "                           tweet.user.username, \n",
    "                           tweet.user.followersCount, \n",
    "                           tweet.hashtags, \n",
    "                           tweet.cashtags, \n",
    "                           tweet.lang])\n",
    "    \n",
    "    df_tweets = pd.DataFrame(tweet_list, columns=['Datetime', # Creating df of tweet data\n",
    "                                                  'Tweet Id', \n",
    "                                                  'Text', \n",
    "                                                  'Username', \n",
    "                                                  'Followers Count', \n",
    "                                                  'Hashtags', \n",
    "                                                  'Cashtags', \n",
    "                                                  'Language'])\n",
    "    \n",
    "    df_tweets.to_csv(file_path, index=False) # Writing df_tweets into new csv file\n",
    "    \n",
    "    if os.path.isfile(file_path) == True:\n",
    "        return print(f'Successfully saved DataFrame to {file_path}')\n",
    "    else:\n",
    "        return print('DataFrame not saved -- possible error has occurred.')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5dfebb5-6eb1-4b19-8686-9dcc43579714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to clean dataframes programatically\n",
    "# Note: it might be a good idea to put scraping, saving, and cleaning into one function?\n",
    "# Check the integrity of each wrangled dataframe just in case ***\n",
    "\n",
    "\n",
    "def dataWrangle(dataframe_list):\n",
    "    \"\"\"Input list of dataframes and start process via concatonation\n",
    "        Language = English\n",
    "        Columns = Datetime, Tweet Id, Text, Username, Followers Count\n",
    "        \n",
    "        Convert Tweet Id type to string to rectify merging issues later\n",
    "        Drop NA\n",
    "        Drop Duplicates\n",
    "        Reset index\n",
    "        \n",
    "        returns cleaned up dataframe\"\"\"\n",
    "    \n",
    "    df_concat = pd.concat(dataframe_list) # Concatenate all the DataFrames from the list of DataFrames\n",
    "    df_filter = df_concat[df_concat['Language'] == 'en'][['Datetime', # Filter via Language = 'en'\n",
    "                                                           'Tweet Id', # Remove unwanted columns\n",
    "                                                           'Text', \n",
    "                                                           'Username',\n",
    "                                                           'Followers Count']] \n",
    "    df_clean = df_filter.astype({'Tweet Id':str})\\\n",
    "                        .dropna()\\\n",
    "                        .drop_duplicates()\\\n",
    "                        .reset_index(drop=True)\n",
    "    \n",
    "    df_clean['Text'] = (df_clean['Text'] # Cleans out redundant string characters within each tweet\n",
    "                       .apply(lambda x: ' '.join(re.sub(r'https\\S+', '', x)\n",
    "                                                .replace('\\n', ' ')\n",
    "                                                .split()\n",
    "                                                )\n",
    "                             )\n",
    "                       )\n",
    "    \n",
    "    return df_clean\n",
    "                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "434410a9-9c53-429f-9e88-64eae627eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file names\n",
    "\n",
    "# Files from two directories -- scraped at different time irl \n",
    "# for the same dates within the data\n",
    "\n",
    "file_path_tweets_GME = glob.glob('tweets_GME/*')\n",
    "file_path_tweets_scraped = glob.glob('tweets_scraped/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cf0313a-1ff8-4be7-9e0a-9fb3c5c289bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of DataFrames using glob's list of files names\n",
    "df_list = []\n",
    "for i in file_path_tweets_GME + file_path_tweets_scraped:\n",
    "    df_list.append(pd.read_csv(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf82f3f-f0a5-48eb-a84e-e2fac6d89e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = date(2021, 1, 8).strftime('%Y-%m-%d')\n",
    "stop_time = date(2021, 2, 12).strftime('%Y-%m-%d')\n",
    "\n",
    "def toDateTimeIndex(df): # year-month-day\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime']).dt.floor('d').dt.tz_localize(tz=None)\n",
    "    df.index = pd.DatetimeIndex(df['Datetime'])\n",
    "    df = df.drop(columns=['Datetime'])\n",
    "    return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83032170-f296-4f59-bfd6-80b42e4cfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = dataWrangle(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1436eb3-5c70-45b9-94f6-8d78be4ed257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Followers Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-08 23:18:18+00:00</td>\n",
       "      <td>1347684080905814016</td>\n",
       "      <td>$GME NEW ARTICLE : GameStop Is Caught in a Vic...</td>\n",
       "      <td>StckPro</td>\n",
       "      <td>4198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-08 23:17:53+00:00</td>\n",
       "      <td>1347683977327497217</td>\n",
       "      <td>@RamBhupatiraju @richard_chu97 @saxena_puru @F...</td>\n",
       "      <td>tmyrbrgh</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-08 23:08:32+00:00</td>\n",
       "      <td>1347681621953159169</td>\n",
       "      <td>GameStop Is Caught in a Vicious Cycle $GME $TG...</td>\n",
       "      <td>newsfilterio</td>\n",
       "      <td>20861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-08 20:14:02+00:00</td>\n",
       "      <td>1347637710866030592</td>\n",
       "      <td>@michaeljburry what are your thought on what s...</td>\n",
       "      <td>JohnMOFOThomas</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-08 18:56:31+00:00</td>\n",
       "      <td>1347618202612760576</td>\n",
       "      <td>@ryancohen Can't stop, won't stop, GameStop! C...</td>\n",
       "      <td>AeternumLibera</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106623</th>\n",
       "      <td>2021-02-05 14:57:54+00:00</td>\n",
       "      <td>1357705012919508995</td>\n",
       "      <td>@carlquintanilla @CNBC They Stopped Us Again, ...</td>\n",
       "      <td>BleezyforSheezy</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106624</th>\n",
       "      <td>2021-02-05 14:57:54+00:00</td>\n",
       "      <td>1357705010222673923</td>\n",
       "      <td>$GME said</td>\n",
       "      <td>prodigenoir</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106625</th>\n",
       "      <td>2021-02-05 14:57:53+00:00</td>\n",
       "      <td>1357705006724620290</td>\n",
       "      <td>$GME and $AMC halted 🤨</td>\n",
       "      <td>tweek3634</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106626</th>\n",
       "      <td>2021-02-05 14:57:52+00:00</td>\n",
       "      <td>1357705004996517891</td>\n",
       "      <td>BUY #AMC and #GME 💎🙌🚀🦍🍌 (not financial advice)</td>\n",
       "      <td>YoSheenn</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106627</th>\n",
       "      <td>2021-02-05 14:57:52+00:00</td>\n",
       "      <td>1357705003385970695</td>\n",
       "      <td>$GME going crazy again 😄</td>\n",
       "      <td>momchev12</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106628 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Datetime             Tweet Id  \\\n",
       "0       2021-01-08 23:18:18+00:00  1347684080905814016   \n",
       "1       2021-01-08 23:17:53+00:00  1347683977327497217   \n",
       "2       2021-01-08 23:08:32+00:00  1347681621953159169   \n",
       "3       2021-01-08 20:14:02+00:00  1347637710866030592   \n",
       "4       2021-01-08 18:56:31+00:00  1347618202612760576   \n",
       "...                           ...                  ...   \n",
       "106623  2021-02-05 14:57:54+00:00  1357705012919508995   \n",
       "106624  2021-02-05 14:57:54+00:00  1357705010222673923   \n",
       "106625  2021-02-05 14:57:53+00:00  1357705006724620290   \n",
       "106626  2021-02-05 14:57:52+00:00  1357705004996517891   \n",
       "106627  2021-02-05 14:57:52+00:00  1357705003385970695   \n",
       "\n",
       "                                                     Text         Username  \\\n",
       "0       $GME NEW ARTICLE : GameStop Is Caught in a Vic...          StckPro   \n",
       "1       @RamBhupatiraju @richard_chu97 @saxena_puru @F...         tmyrbrgh   \n",
       "2       GameStop Is Caught in a Vicious Cycle $GME $TG...     newsfilterio   \n",
       "3       @michaeljburry what are your thought on what s...   JohnMOFOThomas   \n",
       "4       @ryancohen Can't stop, won't stop, GameStop! C...   AeternumLibera   \n",
       "...                                                   ...              ...   \n",
       "106623  @carlquintanilla @CNBC They Stopped Us Again, ...  BleezyforSheezy   \n",
       "106624                                          $GME said      prodigenoir   \n",
       "106625                             $GME and $AMC halted 🤨        tweek3634   \n",
       "106626     BUY #AMC and #GME 💎🙌🚀🦍🍌 (not financial advice)         YoSheenn   \n",
       "106627                           $GME going crazy again 😄        momchev12   \n",
       "\n",
       "       Followers Count  \n",
       "0                 4198  \n",
       "1                  263  \n",
       "2                20861  \n",
       "3                    5  \n",
       "4                   36  \n",
       "...                ...  \n",
       "106623             131  \n",
       "106624             848  \n",
       "106625              24  \n",
       "106626             201  \n",
       "106627             653  \n",
       "\n",
       "[106628 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_kernel",
   "language": "python",
   "name": "capstone_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
